{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data uisng pandas\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"/home/cso/Assgnmt Code/twttr/as_tweets.txt\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace column name\n",
    "data = data.rename(columns={'American Sniper':'tweets'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4312\n"
     ]
    }
   ],
   "source": [
    "ClintEastwood = data[data['tweets'].str.contains(r\"\\bclint\\b|\\beastwood\\b|\\bclinteastwood\\b|\\bclints\\b|\\beastwoods\\b|\\bclinteastwoods\\b\", case = False)]\n",
    "print(len(ClintEastwood))\n",
    "ClintEastwood = ClintEastwood.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of words in each row\n",
    "ClintEastwood['word_count'] = ClintEastwood['tweets'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Number of characters in each row\n",
    "# this also includes spaces\n",
    "ClintEastwood['chr_count'] = ClintEastwood['tweets'].str.len() ## this also includes spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "ClintEastwood['stopword_count'] = ClintEastwood['tweets'].apply(lambda x: len([x for x in x.split() if x in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of special characters\n",
    "ClintEastwood['spl-chr_count'] = ClintEastwood['tweets'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of numerics \n",
    "ClintEastwood['number_count'] = ClintEastwood['tweets'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4312, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>word_count</th>\n",
       "      <th>chr_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>spl-chr_count</th>\n",
       "      <th>number_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American Sniper was truly great. I should have...</td>\n",
       "      <td>26</td>\n",
       "      <td>140</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT \"@Berduu \"American Camper\". Not directed by...</td>\n",
       "      <td>18</td>\n",
       "      <td>139</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intvd Bradley Cooper, Clint Eastwood, Taya Kyl...</td>\n",
       "      <td>18</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The top film of last Friday was again Clint Ea...</td>\n",
       "      <td>14</td>\n",
       "      <td>119</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank u Clint Eastwood &amp; Bradley Cooper for ma...</td>\n",
       "      <td>25</td>\n",
       "      <td>139</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  word_count  chr_count  \\\n",
       "0  American Sniper was truly great. I should have...          26        140   \n",
       "1  RT \"@Berduu \"American Camper\". Not directed by...          18        139   \n",
       "2  Intvd Bradley Cooper, Clint Eastwood, Taya Kyl...          18        140   \n",
       "3  The top film of last Friday was again Clint Ea...          14        119   \n",
       "4  Thank u Clint Eastwood & Bradley Cooper for ma...          25        139   \n",
       "\n",
       "   stopword_count  spl-chr_count  number_count  \n",
       "0              10              0             0  \n",
       "1               2              1             0  \n",
       "2               2              1             0  \n",
       "3               3              0             0  \n",
       "4               7              0             0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ClintEastwood.shape)\n",
    "ClintEastwood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the http links availaible in data\n",
    "import re\n",
    "ClintEastwood['tweets'] = ClintEastwood['tweets'].replace(r'http\\S+','',regex=True).replace(r'https\\S+','',regex=True).replace(r'www\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\n",
    "#Removing Non ASCII characters\n",
    "import unicodedata\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "ClintEastwood['tweets'] = remove_non_ascii(ClintEastwood['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_non_ascii('American Sniper was the best film ever üî•üëåüî´')\n",
    "''.join(remove_non_ascii('ŸÅŸäŸÑŸÖ ÿ•ÿ´ÿßÿ±ÿ© ÿ¨ÿØŸäÿØ ÿ®ÿπŸÜŸàÿßŸÜ¬†Sniper Elite¬† #ÿ≥ŸäŸÜŸÖÿß aaaaaaaaaaaaa„Äê‰ªäÊó•„ÅÆ„Ç¢„É°„É™„Ç´„É≥„Çπ„Éä„Ç§„Éë„Éº„Äë American sniper Âπ≥Êó•ÊòºÈñì„ÅÆÊò†ÁîªÈ§®„ÅØÈñëÊï£„Å®„Åó„Å¶„ÅÑ„Å¶ÊÑâ„Åó„ÅÑ„ÄÇAmerican Sniper was the best film ever üî•üëåüî´'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing all the characters to lower case\n",
    "ClintEastwood['tweets'] = [x.lower() for x in ClintEastwood['tweets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Removing Numbers\n",
    "ClintEastwood['tweets'] = ClintEastwood['tweets'].str.replace('\\d+','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClintEastwood['tweets'][39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuation\n",
    "ClintEastwood['tweets'] = ClintEastwood['tweets'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing english stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "ClintEastwood['tweets'] = ClintEastwood['tweets'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "ClintEastwood['tweets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''.join('rt in wrte'.replace(r'rt',\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rt word, as it represents retweet\n",
    "freq = [\"rt\",\"retweet\"]\n",
    "ClintEastwood['tweets'] = ClintEastwood['tweets'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "ClintEastwood['tweets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize the corpus\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "ClintEastwood['tweets'] = lemmatize_verbs(ClintEastwood['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClintEastwood.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ClintEastwood.to_csv(\"ClintEastwood1.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "high_freq = pd.Series(' '.join(ClintEastwood['tweets']).split()).value_counts()[:30]\n",
    "high_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(high_freq.index)\n",
    "aa = ClintEastwood['tweets'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "high_freq_2 = pd.Series(' '.join(aa).split()).value_counts()[:50]\n",
    "high_freq_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq = pd.Series(' '.join(ClintEastwood['tweets']).split()).value_counts()[-20:]\n",
    "low_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize as nt\n",
    "import nltk\n",
    "text = 'Honestly, American Sniper is probably the closest to combat in a foreign country most of you will ever get. Good job, Clint Eastwood.'\n",
    "# text=\"Being more Pythonic is good for health.\"\n",
    "# text = ClintEastwood['tweets'][1305]\n",
    "ss = nt.sent_tokenize(text)\n",
    "tokenized_sent = [nt.word_tokenize(sent) for sent in ss]\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClintEastwood['tweets'][1308]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# words = ' '.join(list(ClintEastwood['tweets']))\n",
    "words = ' '.join(list(aa))\n",
    "words_wc = WordCloud(width=512,height=512).generate(words)\n",
    "plt.figure(figsize=(10,8), facecolor='k')\n",
    "plt.imshow(words_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words = [stemmer.stem(word) for word in aa]\n",
    "''.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = ''.join(\"I am going to playing in world\")\n",
    "aa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
